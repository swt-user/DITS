# both
initial_model_path: /data/user_data/wentaos/Huggingface_models/Llama-3.1-8B-Instruct
initial_dataset_path: ""
dataset_type: math
mid_yaml_root_path: ./alignment-handbook/recipes/Llama3.1-8B-multi-DI/math_sft_dpo
check_point_root_path: /data/group_data/cx_group/MCTS-agent/checkpoints/math_sft_dpo_llama3.1_DI
iteration_times: 3
port: 8000
devices: 7
tokenizer_first_path: /data/user_data/wentaos/Huggingface_models/Llama-3.1-8B-Instruct
tokenizer_second_path: /data/user_data/wentaos/Huggingface_models/Llama-3.1-8B-Instruct
explore_count: 10
thread_count: 128
prompt_pool_path: ./utils/prompts_math_first.jsonl
# sft
origin_sft_yaml_path: ./alignment-handbook/recipes/Llama3.1-8B-multi-DI/math_sft_dpo/sft/base.yaml
mid_sft_jsonl_root_path: /data/group_data/cx_group/MCTS-agent/results/math_sft_dpo_llama3.1_DI/sft
mid_sft_dataset_root_path: /data/group_data/cx_group/MCTS-agent/my_datasets/math_sft_dpo_llama3.1_DI/sft
initial_episilon: 0.6
sample_count: 7000
# dpo
origin_dpo_yaml_path: ./alignment-handbook/recipes/Llama3.1-8B-multi-DI/math_sft_dpo/dpo/base.yaml
origin_dpo_yaml_path_lora: ./alignment-handbook/recipes/Llama3.1-8B-multi-DI/math_sft_dpo/dpo/base_lora.yaml
mid_dpo_jsonl_root_path: /data/group_data/cx_group/MCTS-agent/results/math_sft_dpo_llama3.1_DI/dpo
mid_dpo_dataset_root_path: /data/group_data/cx_group/MCTS-agent/my_datasets/math_sft_dpo_llama3.1_DI/dpo
initial_dpo_min_value: 0.2
initial_dpo_episilon: 0.45
monte_sample_count: 7000
cal_ppl: 1
lambda1: -0.4
lambda2: 0.9
from_initial: False